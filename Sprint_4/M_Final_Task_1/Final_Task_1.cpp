/*
https://contest.yandex.ru/contest/24414/run-report/79873092/
ПРИНЦИП РАБОТЫ:
Основная идея заключается в том, чтобы собрать частотный словарь встреч каждого слова в каждом документе. Затем каждый запрос обрабатывается поиском по этому частотному словарю и формируется 
метрика релевантности для каждого документа, равная сумме частот встреченных слов в документе. Метрика релевантности и индекс документа записываются в вектор пар со знаком минус, затем он сортируются 
(в лексикографическом смысле) и выводится 5 наиболее релевантных документов.

ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ:
--- СЧИТЫВАНИЕ ДОКУМЕНТОВ ---
1) Создан класс DocumentSearch, содержащий частотный словарь documents: хэш-таблица слов, ключ - слово, значение - хэш-таблица документов с ключом - номером документа (в порядке ввода) и
значением - частотой встречи слова в документе. Число наиболее релевантных документов для вывода - установлено как константа.

Максимальное число документов = 10^4, в каждом документе максимальное возможное число символов = 10^3, слова состоят из 26 маленьких латинских букв и разделены между собой пробелами.
Худшим случаем будет когда в каждом документе будет уникальное слово. Число уникальных слов длины n вычисляется как 26^n. Всего возможно символов 10^4*1001 (чтобы не учитывать пробел в конце),
таким образом доступно 10'010'000 символов с учётом пробела на конце документа. Число уникальных слов длиной n:
n = 1 => 26
n = 2 => 676
n = 3 => 17'576
n = 4 => 456'976
n = 5 => 11'881'376

Поскольку для слова требуется ещё пробел, то число используемых символов для каждого уникального слова будет:

n = 1 => 26 * (1 + 1) = 52
n = 2 => 676 * (2 + 1) = 2'028
n = 3 => 17'576 * (3 + 1) = 70'304
n = 4 => 456'976 * (4 + 1) = 2'284'880
n = 5 => 11'881'376 * (5 + 1) = 71'228'256

Следовательно, худший случай = 10^4 в каждом из которых разные слова, длиной до 5 символов => потребуется (10'010'000 - 52 - 2'028 - 70'304 - 2'284'880) / (5 + 1) ~ 1'275'456 слов длиной не больше 5.
Итого: 26 + 676 + 17'576 + 456'976 + 1'275'456 = 1'750'710 разных слов - худший случай.

Тем не менее, эмпирически установлено, что значение рехеширования в 1019 также дает +- приемлимый результат, как по времени, так и по памяти (воспользуемся поэтому им).

2) После инициализации класса, изначально устанавливается число документов = 0, число релевантных документов указывается константным, создается временная строка для чтения слов;
3) Из ввода считывается число документов n и далее считываются n строчек;
4) Для каждого документа i слова считываются через поток istringstream и каждое слово из потока (возможны повторения) увеличивает свою частоту в документе i на 1. То есть мы линейно идем
по каждому слову в считанной через поток строке и увеличиваем для этого слова его частоту в документе i;
5) После прохождения каждого документа мы получаем словарь с частотой встречи каждого слова в каждом документе;

--- ОБРАБОТКА ЗАПРОСОВ ---
6) Из ввода считывается число запросов m и далее считываются m строчек;
7) Для каждой строчки мы создаем:
	(1) Вектор пар из номера документа и метрики релевантности - для сортировки и вывода релевантных документов;
	(2) Вектор с метрикой релевантности - для подсчета релевантных документов, где индекс метрики = (номер документа - 1);
	(3) Неупорядоченное множество на основе хэша - для контроля за уникальными словами, чтобы мы не учитывали одно и тоже слово при подсчете метрики релевантности;
8) Из потока по строке мы выгружаем слово => смотрим, встречалось ли это слово уже: 
	(1) если встречалось, то смотрим другое слово; 
	(2) если нет, то ищем его в частотном словаре документов:
		a. если слова нет в частотном словаре, то смотрим другое слово;
		b. если слово есть в частотном словаре, то для каждого документа, где это слово есть, мы добавляем частоту этого слова к метрике релевантности в вектор метрик;
9) Пройдя каждое слово в потоке, мы переносим метрику релевантности в вектор пар с парой = {метрика релевантности, - (номер документа + 1)};
10) Сортируем вектор пар => сортировка будет лексикографической, сначала по метрике релевантности, затем по номеру документа в порядке возрастания номера, взятого со знаком минус (т.е. первый появившийся 
документ будет более релевантен, при совпадении метрик);

11) Извлекаем из вектора пар не больше 5 последних релевантных номеров документов (сортировка в порядке возрастания => последний = самые релевантные), у которых метрика релевантности не 0, то есть
что есть какое-то совпадение слов. Помещаем номера документов со знаком плюс в новый вектор, начиная с самого конца вектора пар. Если релевантных документов нет, то возвращаемый вектор будет пустым;

12) После извлечения вектора с номерами документов - выводим все номера;

ВРЕМЕННАЯ СЛОЖНОСТЬ:
-- ДОБАВЛЕНИЕ ДОКУМЕНТОВ --
1) Всего n документов, длина каждого не превышает k. Чтение всех строк документов: O(nk);
2) Рехеширование частотного словаря: O(1) в текущей реализации;
3) Добавление слова в частотный словарь: в худшем случае линейная сложность для каждого символа строки => O(nk), в среднем - константная сложность * длину строки => O(k);
4) Добавление частоты встречи слова в каждом документе: в худшем случае линейная сложность O(n), если слово встречается в каждом из документов - и в среднем - константная сложность O(1);
Временная сложность порядка: ~ O(nk)

-- ПОИСК РЕЛЕВАНТНЫХ ДОКУМЕНТОВ --
1) Всего m запросов, длина каждого не превышает p. Чтение всех строк запросов: O(mp);
2) Составление вектора для релевантности метрик и вектора пар: O(n)
3) Составление неупорядоченного множества для каждого слова из запроса в худшем случай - O(mp), в среднем - обращение по строке O(p);
4) Поиск слова во множестве - худший случай O(mp), в среднем случай - обращение по строке O(p);
5) Поиск слова в частотном словаре - худший случай O(np), в среднем случай - обращение по строке O(p);
6) Сортировка вектора пар: ~O(n log(n));
7) Вывод последних не более, чем 5 элементов: ~O(1);
Временная сложность порядка: ~O(mp + n + np + n log(n)) = O(p(m + n) + nlog(n))

Итого: общая временная сложность O(p(m + n) + nlog(n) + nk)

ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ:

-- ДОБАВЛЕНИЕ ДОКУМЕНТОВ --
1) В худшем случае, в словаре будет хранится n различных строк длины k, в лучшем случае будет занята рехешированная память под 101 ячейку => простр. сложность: ~O(nk)
2) В случае, когда несколько слов будут хранится во всех документах, то их длина будет < k и для каждого слова будет существовать словарь порядка ~O(n), что меньше, чем ~O(nk);
Пространственная сложность порядка: ~ O(nk)

-- ПОИСК РЕЛЕВАНТНЫХ ДОКУМЕНТОВ --
1) В худшем случае, во множестве будет хранится m различных строк длины p, в лучшем случае будет занята рехешированная память под 101 ячейку  => простр. сложность: ~O(mp)
2) Составление вектора для релевантности метрик и вектора пар: ~O(n + 2n) = O(n)
3) Сортировка вектора пар в худшем случае: ~O(n);
4) Вывод последних не более, чем 5 элементов: ~O(1);
Пространственная сложность порядка: ~O(mp + n)

Итого: общая пространственная сложность O(mp + n + nk) = O(mp + nk)

*/

#include<iostream>
#include<unordered_map>
#include<unordered_set>
#include<vector>
#include<string>
#include<sstream>
#include<algorithm>
using namespace std;
using ConstRevIterator = vector<pair<int, int>>::const_reverse_iterator;
constexpr size_t documentsHashSize = 1019;
constexpr size_t queryHashSize = 101;
constexpr size_t relevantDocs = 5;

class DocumentSearch {
public:
	DocumentSearch() {
		docsNum = 0;
		relevanceSize = relevantDocs;
		documents.rehash(documentsHashSize);
		tempWord = "";
	}

	void readDocument(istringstream& iss) {

		while (iss >> tempWord) {
			documents[tempWord][docsNum]++;
		}
		docsNum++;
	}

	vector<int> findRelevantDocuments(istringstream& iss) {

		// Вектор для подсчета релевантности документов
		vector<int> docCount(docsNum, 0);

		// Вектор для сортировки и вывода релевантных документов 
		vector<pair<int, int>> docCountPair(docsNum, { 0,0 });

		// Неупорядоченное множество встреченных слов (хэш-таблица)
		unordered_set<string> wordsQueryHashTab;
		wordsQueryHashTab.rehash(queryHashSize);

		while (iss >> tempWord) {

			// Добавление слова в хэш-таблицу
			auto it_word = wordsQueryHashTab.find(tempWord);

			// Если слово уже было, то смотрим другое слово
			if (it_word != wordsQueryHashTab.end()) { continue; }
			wordsQueryHashTab.insert(tempWord);

			// Поиск слова по частотному словарю
			auto wordInDocs = documents.find(tempWord);

			// Если слова нет в словаре, смотрим следующее слово
			if (wordInDocs == documents.end()) { continue; }
			auto docsFromWord = wordInDocs->second;

			// Подсчет количества вхождений слова в каждый документ
			for (auto itDocs = docsFromWord.begin(); itDocs != docsFromWord.end(); itDocs++) {
				docCount[itDocs->first] += itDocs->second;
			}
		}

		for (size_t j = 0; j < docsNum; j++) {
			int nj = j + 1;
			docCountPair[j] = { docCount[j],-nj  };
		}

		// Сортируем лексикографически вектор: сначала по метрике релевантности, затем по убыванию порядковых номеров со знаком минус (по возрастанию обычных номеров)
		sort(docCountPair.begin(), docCountPair.end());
		return prepareRelevant(docCountPair.rbegin(), docCountPair.rend());

	}

	vector<int> prepareRelevant(ConstRevIterator begin, ConstRevIterator end) {
		vector<int> result;

		size_t counter = 0;
		for (auto itDoc = begin; itDoc != end; itDoc++) {
			if (itDoc->first == 0) { break; }
			if (counter == relevanceSize) { break; }

			result.push_back(-itDoc->second );
			counter++;
		}
		return result;
	}

private:
	string tempWord;
	size_t docsNum, relevanceSize;
	unordered_map<string, unordered_map<int, int>> documents;
};

void printVector(const vector<int>& vec) {
	for (auto it = vec.begin(); it != vec.end(); it++) {
		cout << *it << " ";
	}
	cout << "\n";
}


int main() {
	ios_base::sync_with_stdio(false);
	cin.tie(NULL);

	size_t n, m;
	string tempLine;
	DocumentSearch docs;
	vector<int> result;

	cin >> n;
	cin.get();

	for (size_t i = 0; i < n; i++) {
		getline(cin, tempLine);
		istringstream iss(tempLine);
		docs.readDocument(iss);
	}

	cin >> m;
	cin.get();

	for (size_t j = 0; j < m; j++) {
		getline(cin, tempLine);
		istringstream iss(tempLine);
		result = docs.findRelevantDocuments(iss);
		printVector(result);
	}

	return 0;
}